

<!doctype html>
    <html lang="en-US" id="template-archives" class="base">
        <head>
        
        <script defer data-domain="fightwithtools.dev" src="https://plausible.io/js/plausible.js"></script>
        <link rel="alternate" type="application/rss+xml" title="Fight With Tools: A Dev Blog RSS" href="https://fightwithtools.dev/rss/" />
    	<link rel="alternate" type="application/atom+xml" title="Fight With Tools: A Dev Blog Feed" href="https://fightwithtools.dev/feed/" />
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns</title>
        
            <!-- Sub Template: archives -->
        
        <link rel="original" http-equiv href="https://github.com/ArchiveTeam/grab-site" />
        
            
                <!-- <link rel="stylesheet" href="https://fightwithtools.dev/assets/css/template-archives.css?v=2412017517" /> -->
                <link rel="stylesheet" href="https://fightwithtools.dev/assets/css/template-archives.css?v=2412017517" id=""></link>

            
        
        
            <link rel="canonical" href="https://github.com/ArchiveTeam/grab-site" />
            

            <meta property="og:title"
                content="GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns">

            <meta property="og:description"
                content="The archivist&#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns - GitHub - ArchiveTeam/grab-site: The archivist&#39;s web...">


            <meta property="og:url"
                content="https://github.com/ArchiveTeam/grab-site" />

            <meta property="og:site_name" content="Fight With Tools: A Dev Blog" />

            <meta property="og:locale" content="en_US" />

            <meta name="twitter:site" content="@chronotope" />

            <meta name="twitter:description" content="The archivist&#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns - GitHub - ArchiveTeam/grab-site: The archivist&#39;s web..." />

            <!-- I prefer the summary_large_image Twitter card for posts. -->
            <meta name="twitter:card" content="summary_large_image" />
            <!-- You, you're the creator. -->
            <meta name="twitter:creator" content="@chronotope" />
            <!-- This property is for the article title, not site title. -->
            <meta name="twitter:title" content="GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns" />

            

            <meta property="og:type" content="website" />

            <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "url": "https://github.com/ArchiveTeam/grab-site",
                    "headline": "GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns",
                    "about": "The archivist&#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns - GitHub - ArchiveTeam/grab-site: The archivist&#39;s web crawler: WARC output, dashboard for all cra...",

                    "image": [
                        
                        
                    ],
                    "isAccessibleForFree": "True",
                    "isPartOf": {
                        "@type": ["CreativeWork", "Product"],
                        "name": "Fight With Tools",
                        "productID": "fightwithtools.dev"
                    },
                    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
                    "archivedAt": "https://fightwithtools.dev/timegate/https:/github.com/ArchiveTeam/grab-site/",
                    "sameAs": "https://github.com/ArchiveTeam/grab-site",
                }
            </script>


        
        
        </head>
        <body >
            <div class="wrapper">
                <header>
                    <div class="left">
                        <h1>Archived: GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns</h1>
                        <p>This is a simplified archive of the page at <a href="https://github.com/ArchiveTeam/grab-site" target="_blank">https://github.com/ArchiveTeam/grab-site</a></p>
                    </div>

                            <div id="share-embed">
                                <span>Use this page embed on your own site:</span>
                                <textarea id="html-embed" name="story" rows="8" cols="40">
                                    <script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute("target","_blank")}}customElements.define("contexter-link",ContexterLink,{extends:"a"}),customElements.define("contexter-inner",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__inner"}}),customElements.define("contexter-thumbnail",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__thumbnail"}}),customElements.define("contexter-byline",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__byline"}}),customElements.define("contexter-keywordset",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__keywordset"}}),customElements.define("contexter-linkset",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__linkset"}}),customElements.define("contexter-meta",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__meta"}}),customElements.define("contexter-summary",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="p-summary entry-summary"}}),customElements.define("contexter-box-head",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className="contexter-box__head"}}),customElements.define("contexter-box-inner",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:"open"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement("style"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: "|";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement("style"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement("contexter-box-inner"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement("slot")),innerSlotHeader=(innerSlotThumbnail.name="thumbnail",innerContainer.appendChild(innerSlotThumbnail),document.createElement("slot")),innerSlotAuthor=(innerSlotHeader.name="header",innerContainer.appendChild(innerSlotHeader),document.createElement("slot")),innerSlotTime=(innerSlotAuthor.name="author",innerContainer.appendChild(innerSlotAuthor),document.createElement("slot")),innerSlotSummary=(innerSlotTime.name="time",innerContainer.appendChild(innerSlotTime),document.createElement("slot")),metaContainer=(innerSlotSummary.name="summary",innerContainer.appendChild(innerSlotSummary),document.createElement("contexter-meta")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement("slot")),linkContainer=(innerSlotInfo.name="keywords",metaContainer.appendChild(innerSlotInfo),document.createElement("contexter-linkset")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement("slot")),innerSlotReadLink=(innerSlotArchiveLink.name="archive-link",linkContainer.appendChild(innerSlotArchiveLink),document.createElement("slot"));innerSlotReadLink.name="read-link",linkContainer.appendChild(innerSlotReadLink),this.className="contexter-box",this.onclick=e=>{if(!e.target.className.includes("read-link")&&!e.target.className.includes("title-link")){const mainLinks=this.querySelectorAll("a.main-link");mainLinks[0].click()}}}}}customElements.define("contexter-box",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class="link-card h-entry hentry" itemscope="" itemtype="https://schema.org/CreativeWork"><contexter-thumbnail class="thumbnail" slot="thumbnail"></contexter-thumbnail><contexter-box-head slot="header" class="p-name entry-title" itemprop="headline"><contexter-box-head slot="header" class="p-name entry-title" itemprop="headline"><a is="contexter-link" href="https://github.com/ArchiveTeam/grab-site" itemprop="url">GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns</a></contexter-box-head></contexter-box-head><time class="dt-published published" slot="time" itemprop="datePublished" datetime="2022-02-21T18:33:56.258Z">1/21/2022</time><contexter-summary class="p-summary entry-summary" itemprop="abstract" slot="summary"><p>The archivist&#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns - GitHub - ArchiveTeam/grab-site: The archivist&#39;s web crawler: WARC output, dashboard for all cra...</p></contexter-summary><contexter-keywordset itemprop="keywords" slot="keywords"></contexter-keywordset><a is="contexter-link" href="https://github.com/ArchiveTeam/grab-site" class="read-link main-link" itemprop="sameAs" slot="read-link">Read</a><a href="https://fightwithtools.dev/timegate/https://github.com/ArchiveTeam/grab-site" is="contexter-link" target="_blank" class="read-link archive-link" itemprop="archivedAt" slot="archive-link">Archived</a></contexter-box>
                                </textarea><br />
                                <button onclick="(function(e){ navigator.clipboard.writeText(document.getElementById('html-embed').value); })()">Copy</button>
                            </div>
                            
                            <script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute("target","_blank")}}customElements.define("contexter-link",ContexterLink,{extends:"a"}),customElements.define("contexter-inner",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__inner"}}),customElements.define("contexter-thumbnail",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__thumbnail"}}),customElements.define("contexter-byline",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__byline"}}),customElements.define("contexter-keywordset",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__keywordset"}}),customElements.define("contexter-linkset",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__linkset"}}),customElements.define("contexter-meta",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="contexter-box__meta"}}),customElements.define("contexter-summary",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className="p-summary entry-summary"}}),customElements.define("contexter-box-head",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className="contexter-box__head"}}),customElements.define("contexter-box-inner",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:"open"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement("style"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: "|";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement("style"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement("contexter-box-inner"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement("slot")),innerSlotHeader=(innerSlotThumbnail.name="thumbnail",innerContainer.appendChild(innerSlotThumbnail),document.createElement("slot")),innerSlotAuthor=(innerSlotHeader.name="header",innerContainer.appendChild(innerSlotHeader),document.createElement("slot")),innerSlotTime=(innerSlotAuthor.name="author",innerContainer.appendChild(innerSlotAuthor),document.createElement("slot")),innerSlotSummary=(innerSlotTime.name="time",innerContainer.appendChild(innerSlotTime),document.createElement("slot")),metaContainer=(innerSlotSummary.name="summary",innerContainer.appendChild(innerSlotSummary),document.createElement("contexter-meta")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement("slot")),linkContainer=(innerSlotInfo.name="keywords",metaContainer.appendChild(innerSlotInfo),document.createElement("contexter-linkset")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement("slot")),innerSlotReadLink=(innerSlotArchiveLink.name="archive-link",linkContainer.appendChild(innerSlotArchiveLink),document.createElement("slot"));innerSlotReadLink.name="read-link",linkContainer.appendChild(innerSlotReadLink),this.className="contexter-box",this.onclick=e=>{if(!e.target.className.includes("read-link")&&!e.target.className.includes("title-link")){const mainLinks=this.querySelectorAll("a.main-link");mainLinks[0].click()}}}}}customElements.define("contexter-box",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class="link-card h-entry hentry" itemscope="" itemtype="https://schema.org/CreativeWork"><contexter-thumbnail class="thumbnail" slot="thumbnail"></contexter-thumbnail><contexter-box-head slot="header" class="p-name entry-title" itemprop="headline"><contexter-box-head slot="header" class="p-name entry-title" itemprop="headline"><a is="contexter-link" href="https://github.com/ArchiveTeam/grab-site" itemprop="url">GitHub - ArchiveTeam/grab-site: The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns</a></contexter-box-head></contexter-box-head><time class="dt-published published" slot="time" itemprop="datePublished" datetime="2022-02-21T18:33:56.258Z">1/21/2022</time><contexter-summary class="p-summary entry-summary" itemprop="abstract" slot="summary"><p>The archivist&#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns - GitHub - ArchiveTeam/grab-site: The archivist&#39;s web crawler: WARC output, dashboard for all cra...</p></contexter-summary><contexter-keywordset itemprop="keywords" slot="keywords"></contexter-keywordset><a is="contexter-link" href="https://github.com/ArchiveTeam/grab-site" class="read-link main-link" itemprop="sameAs" slot="read-link">Read</a><a href="https://fightwithtools.dev/timegate/https://github.com/ArchiveTeam/grab-site" is="contexter-link" target="_blank" class="read-link archive-link" itemprop="archivedAt" slot="archive-link">Archived</a></contexter-box>
                            


                </header>
                <main>
                
                    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text"><h2 dir="auto">grab-site</h2>
<p dir="auto"><a href="https://travis-ci.org/ArchiveTeam/grab-site" rel="nofollow"><img src="https://camo.githubusercontent.com/d1fc302b67c288053876c6658493360ea580169b74c3a516dc86b92bbea50ed4/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f417263686976655465616d2f677261622d736974652e737667" alt="Build status" data-canonical-src="https://img.shields.io/travis/ArchiveTeam/grab-site.svg"></a></p>
<p dir="auto">grab-site is an easy preconfigured web crawler designed for backing up websites.
Give grab-site a URL and it will recursively crawl the site and write
<a href="https://www.archiveteam.org/index.php?title=The_WARC_Ecosystem" rel="nofollow">WARC files</a>.
Internally, grab-site uses <a href="https://github.com/ludios/wpull">a fork</a> of
<a href="https://github.com/chfoo/wpull">wpull</a> for crawling.</p>
<p dir="auto">grab-site gives you</p>
<ul dir="auto">
<li>
<p dir="auto">a dashboard with all of your crawls, showing which URLs are being
grabbed, how many URLs are left in the queue, and more.</p>
</li>
<li>
<p dir="auto">the ability to add ignore patterns when the crawl is already running.
This allows you to skip the crawling of junk URLs that would
otherwise prevent your crawl from ever finishing.  See below.</p>
</li>
<li>
<p dir="auto">an extensively tested default ignore set (<a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/global">global</a>)
as well as additional (optional) ignore sets for forums, reddit, etc.</p>
</li>
<li>
<p dir="auto">duplicate page detection: links are not followed on pages whose
content duplicates an already-seen page.</p>
</li>
</ul>
<p dir="auto">The URL queue is kept on disk instead of in memory.  If you're really lucky,
grab-site will manage to crawl a site with ~10M pages.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/ArchiveTeam/grab-site/master/images/dashboard.png"><img src="https://raw.githubusercontent.com/ArchiveTeam/grab-site/master/images/dashboard.png" alt="dashboard screenshot"></a></p>
<p dir="auto">Note: if you have any problems whatsoever installing or getting grab-site to run,
please <a href="https://github.com/ArchiveTeam/grab-site/issues">file an issue</a> - thank you!</p>
<p dir="auto"><strong>Contents</strong></p>
<ul dir="auto">
<li><a href="#install-on-ubuntu-1604-1804-2004-debian-9-stretch-debian-10-buster-debian-11-bullseye">Install on Ubuntu 16.04, 18.04, 20.04, Debian 9 (stretch), Debian 10 (buster), Debian 11 (bullseye)</a></li>
<li><a href="#install-on-nixos">Install on NixOS</a></li>
<li><a href="#install-on-another-distribution-lacking-python-37x-or-38x">Install on another distribution lacking Python 3.7.x or 3.8.x</a></li>
<li><a href="#install-on-macos">Install on macOS</a></li>
<li><a href="#install-on-windows-10-experimental">Install on Windows 10 (experimental)</a></li>
<li><a href="#upgrade-an-existing-install">Upgrade an existing install</a></li>
<li><a href="#usage">Usage</a>
<ul dir="auto">
<li><a href="#grab-site-options-ordered-by-importance"><code>grab-site</code> options, ordered by importance</a></li>
<li><a href="#warnings">Warnings</a></li>
<li><a href="#tips-for-specific-websites">Tips for specific websites</a></li>
</ul>
</li>
<li><a href="#changing-ignores-during-the-crawl">Changing ignores during the crawl</a></li>
<li><a href="#inspecting-the-url-queue">Inspecting the URL queue</a></li>
<li><a href="#preventing-a-crawl-from-queuing-any-more-urls">Preventing a crawl from queuing any more URLs</a></li>
<li><a href="#stopping-a-crawl">Stopping a crawl</a></li>
<li><a href="#advanced-gs-server-options">Advanced <code>gs-server</code> options</a></li>
<li><a href="#viewing-the-content-in-your-warc-archives">Viewing the content in your WARC archives</a></li>
<li><a href="#inspecting-warc-files-in-the-terminal">Inspecting WARC files in the terminal</a></li>
<li><a href="#automatically-pausing-grab-site-processes-when-free-disk-is-low">Automatically pausing grab-site processes when free disk is low</a></li>
<li><a href="#thanks">Thanks</a></li>
<li><a href="#help">Help</a></li>
</ul>
<h2 dir="auto">Install on Ubuntu 16.04, 18.04, 20.04, Debian 9 (stretch), Debian 10 (buster), Debian 11 (bullseye)</h2>
<ol dir="auto">
<li>
<p dir="auto">On Debian, use <code>su</code> to become root if <code>sudo</code> is not configured to give you access.</p>
<div data-snippet-clipboard-copy-content="sudo apt-get update
sudo apt-get install --no-install-recommends \
    git build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev \
    libsqlite3-dev libffi-dev libxml2-dev libxslt1-dev libre2-dev pkg-config"><pre><code>sudo apt-get update
sudo apt-get install --no-install-recommends \
    git build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev \
    libsqlite3-dev libffi-dev libxml2-dev libxslt1-dev libre2-dev pkg-config
</code></pre></div>
<p dir="auto">If you see <code>Unable to locate package</code>, run the two commands again.</p>
</li>
<li>
<p dir="auto">As a <strong>non-root</strong> user:</p>
<div data-snippet-clipboard-copy-content="wget https://raw.githubusercontent.com/pyenv/pyenv-installer/master/bin/pyenv-installer
chmod +x pyenv-installer
./pyenv-installer
~/.pyenv/bin/pyenv install 3.8.12
~/.pyenv/versions/3.8.12/bin/python -m venv ~/gs-venv
~/gs-venv/bin/pip install --no-binary lxml --upgrade git+https://github.com/ArchiveTeam/grab-site"><pre><code>wget https://raw.githubusercontent.com/pyenv/pyenv-installer/master/bin/pyenv-installer
chmod +x pyenv-installer
./pyenv-installer
~/.pyenv/bin/pyenv install 3.8.12
~/.pyenv/versions/3.8.12/bin/python -m venv ~/gs-venv
~/gs-venv/bin/pip install --no-binary lxml --upgrade git+https://github.com/ArchiveTeam/grab-site
</code></pre></div>
<p dir="auto"><code>--no-binary lxml</code> is necessary for the html5-parser build.</p>
</li>
<li>
<p dir="auto">Add this to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p>
<div data-snippet-clipboard-copy-content="PATH=&quot;$PATH:$HOME/gs-venv/bin&quot;"><pre><code>PATH="$PATH:$HOME/gs-venv/bin"
</code></pre></div>
<p dir="auto">and then restart your shell (e.g. by opening a new terminal tab/window).</p>
</li>
</ol>
<h2 dir="auto">Install on NixOS</h2>
<p dir="auto">As a <strong>non-root</strong> user:</p>
<div data-snippet-clipboard-copy-content="nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site"><pre><code>nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site
</code></pre></div>
<h2 dir="auto">Install on another distribution lacking Python 3.7.x or 3.8.x</h2>
<p dir="auto">grab-site and its dependencies are available in <a href="https://github.com/NixOS/nixpkgs">nixpkgs</a>, which can be used on any Linux distribution.</p>
<ol dir="auto">
<li>
<p dir="auto">As root:</p>
<p dir="auto">Where <code>USER</code> is your non-root username:</p>
<div data-snippet-clipboard-copy-content="mkdir /nix
chown USER:USER /nix"><pre><code>mkdir /nix
chown USER:USER /nix
</code></pre></div>
</li>
<li>
<p dir="auto">As the <strong>non-root</strong> user, install Nix: <a href="https://nixos.org/nix/download.html" rel="nofollow">https://nixos.org/nix/download.html</a></p>
</li>
<li>
<p dir="auto">As the <strong>non-root</strong> user:</p>
<div data-snippet-clipboard-copy-content="nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site"><pre><code>nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site
</code></pre></div>
<p dir="auto">and then restart your shell (e.g. by opening a new terminal tab/window).</p>
</li>
</ol>
<h2 dir="auto">Install on macOS</h2>
<p dir="auto">On OS X 10.10 - macOS 11:</p>
<ol dir="auto">
<li>
<p dir="auto">Run <code>locale</code> in your terminal.  If the output includes "UTF-8", you
are all set.  If it does not, your terminal is misconfigured and grab-site
will fail to start.  This can be corrected with:</p>
<ul dir="auto">
<li>
<p dir="auto">Terminal.app: Preferences... -&gt; Profiles -&gt; Advanced -&gt; <strong>check</strong> Set locale environment variables on startup</p>
</li>
<li>
<p dir="auto">iTerm2: Preferences... -&gt; Profiles -&gt; Terminal -&gt; Environment -&gt; <strong>check</strong> Set locale variables automatically</p>
</li>
</ul>
</li>
</ol>
<h3 dir="auto">Using Homebrew</h3>
<ol start="2" dir="auto">
<li>
<p dir="auto">Install Homebrew using the install step on <a href="https://brew.sh/" rel="nofollow">https://brew.sh/</a></p>
</li>
<li>
<p dir="auto">Run:</p>
<div data-snippet-clipboard-copy-content="brew update
brew install python@3.7 libxslt re2 pkg-config
/usr/local/opt/python@3.7/bin/python3 -m venv ~/gs-venv
PKG_CONFIG_PATH=&quot;/usr/local/opt/libxml2/lib/pkgconfig&quot; ~/gs-venv/bin/pip install --no-binary lxml --upgrade git+https://github.com/ArchiveTeam/grab-site"><pre><code>brew update
brew install python@3.7 libxslt re2 pkg-config
/usr/local/opt/python@3.7/bin/python3 -m venv ~/gs-venv
PKG_CONFIG_PATH="/usr/local/opt/libxml2/lib/pkgconfig" ~/gs-venv/bin/pip install --no-binary lxml --upgrade git+https://github.com/ArchiveTeam/grab-site
</code></pre></div>
</li>
<li>
<p dir="auto">To put the <code>grab-site</code> binaries in your PATH, add this to your <code>~/.zshrc</code> (macOS 10.15, 11+) or <code>~/.bash_profile</code> (earlier):</p>
<div data-snippet-clipboard-copy-content="PATH=&quot;$PATH:$HOME/gs-venv/bin&quot;"><pre><code>PATH="$PATH:$HOME/gs-venv/bin"
</code></pre></div>
<p dir="auto">and then restart your shell (e.g. by opening a new terminal tab/window).</p>
</li>
</ol>
<h3 dir="auto">Using Nix</h3>
<p dir="auto">As an alternative to the Homebrew install, if you prefer Nix.</p>
<ol start="2" dir="auto">
<li>
<p dir="auto">Install Nix: <a href="https://nixos.org/nix/download.html" rel="nofollow">https://nixos.org/nix/download.html</a></p>
<p dir="auto">On macOS 10.15 or 11+, you will instead need:</p>
<div data-snippet-clipboard-copy-content="sh <(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store-volume"><pre><code>sh &lt;(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store-volume
</code></pre></div>
<p dir="auto">To put <code>nix-env</code> in your PATH, add this to your <code>~/.zshrc</code> (macOS 10.15, 11+) or <code>~/.bash_profile</code> (earlier):</p>
<div data-snippet-clipboard-copy-content=". ~/.nix-profile/etc/profile.d/nix.sh"><pre><code>. ~/.nix-profile/etc/profile.d/nix.sh
</code></pre></div>
<p dir="auto">and then restart your shell (e.g. by opening a new terminal tab/window).</p>
</li>
<li>
<p dir="auto"><code>nix-env</code> is now available to install grab-site. Run:</p>
<div data-snippet-clipboard-copy-content="nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site"><pre><code>nix-env -f https://github.com/NixOS/nixpkgs/archive/release-21.11.tar.gz -iA grab-site
</code></pre></div>
<p dir="auto">and then restart your shell.</p>
</li>
</ol>
<h2 dir="auto">Install on Windows 10 (experimental)</h2>
<p dir="auto">On Windows 10 Fall Creators Update (1703) or newer:</p>
<ol dir="auto">
<li>
<p dir="auto">Start menu -&gt; search "feature" -&gt; Turn Windows features on or off</p>
</li>
<li>
<p dir="auto">Scroll down, check "Windows Subsystem for Linux" and click OK.</p>
</li>
<li>
<p dir="auto">Wait for install and click "Restart now"</p>
</li>
<li>
<p dir="auto">Start menu -&gt; Store</p>
</li>
<li>
<p dir="auto">Search for "Ubuntu" in the store and install Ubuntu (publisher: Canonical Group Limited).</p>
</li>
<li>
<p dir="auto">Start menu -&gt; Ubuntu</p>
</li>
<li>
<p dir="auto">Wait for install and create a user when prompted.</p>
</li>
<li>
<p dir="auto">Follow the <a href="#install-on-ubuntu-1604-1804-2004-debian-9-stretch-debian-10-buster">Ubuntu 16.04, 18.04, 20.04, Debian 9 (stretch), Debian 10 (buster)</a> steps.</p>
</li>
</ol>
<h2 dir="auto">Upgrade an existing install</h2>
<p dir="auto">To update grab-site, simply run the <code>~/gs-venv/bin/pip install ...</code> or
<code>nix-env ...</code> command used to install it originally (see above).</p>
<p dir="auto">After upgrading, stop <code>gs-server</code> with <code>kill</code> or ctrl-c, then start it again.
Existing <code>grab-site</code> crawls will automatically reconnect to the new server.</p>
<h2 dir="auto">Usage</h2>
<p dir="auto">First, start the dashboard with:</p>

<p dir="auto">and point your browser to <a href="http://127.0.0.1:29000/" rel="nofollow">http://127.0.0.1:29000/</a></p>
<p dir="auto">Note: gs-server listens on all interfaces by default, so you can reach the
dashboard by a non-localhost IP as well, e.g. a LAN or WAN IP.  (Sub-note:
no code execution capabilities are exposed on any interface.)</p>
<p dir="auto">Then, start as many crawls as you want with:</p>

<p dir="auto">Do this inside tmux unless they're very short crawls.
Note that <a href="https://github.com/tmux/tmux/issues/298">tmux 2.1 is broken and will lock up frequently</a>.
Ubuntu 16.04 users probably need to remove tmux 2.1 and
<a href="https://gist.github.com/ivan/42597ad48c9f10cdd3c05418210e805b">install tmux 1.8 from Ubuntu 14.04</a>.
If you are unable to downgrade tmux, detaching immediately after starting the
crawl may be enough to avoid the problem.</p>
<p dir="auto">grab-site outputs WARCs, logs, and control files to a new subdirectory in the
directory from which you launched <code>grab-site</code>, referred to here as "DIR".
(Use <code>ls -lrt</code> to find it.)</p>
<p dir="auto">You can pass multiple <code>URL</code> arguments to include them in the same crawl,
whether they are on the same domain or different domains entirely.</p>
<p dir="auto">warcprox users: <a href="https://github.com/internetarchive/warcprox">warcprox</a> breaks the
dashboard's WebSocket; please make your browser skip the proxy for whichever
host/IP you're using to reach the dashboard.</p>
<h3 dir="auto"><code>grab-site</code> options, ordered by importance</h3>
<p dir="auto">Options can come before or after the URL.</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--1</code>: grab just <code>URL</code> and its page requisites, without recursing.</p>
</li>
<li>
<p dir="auto"><code>--igsets=IGSET1,IGSET2</code>: use ignore sets <code>IGSET1</code> and <code>IGSET2</code>.</p>
<p dir="auto">Ignore sets are used to avoid requesting junk URLs using a pre-made set of
regular expressions.  See <a href="https://github.com/ArchiveTeam/grab-site/tree/master/libgrabsite/ignore_sets">the full list of available ignore sets</a>.</p>
<p dir="auto">The <a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/global">global</a>
ignore set is implied and always enabled.</p>
<p dir="auto">The ignore sets can be changed during the crawl by editing the <code>DIR/igsets</code> file.</p>
</li>
<li>
<p dir="auto"><code>--no-offsite-links</code>: avoid following links to a depth of 1 on other domains.</p>
<p dir="auto">grab-site always grabs page requisites (e.g. inline images and stylesheets), even if
they are on other domains.  By default, grab-site also grabs linked pages to a depth
of 1 on other domains.  To turn off this behavior, use <code>--no-offsite-links</code>.</p>
<p dir="auto">Using <code>--no-offsite-links</code> may prevent all kinds of useful images, video, audio, downloads,
etc from being grabbed, because these are often hosted on a CDN or subdomain, and
thus would otherwise not be included in the recursive crawl.</p>
</li>
<li>
<p dir="auto"><code>-i</code> / <code>--input-file</code>: Load list of URLs-to-grab from a local file or from a
URL; like <code>wget -i</code>.  File must be a newline-delimited list of URLs.
Combine with <code>--1</code> to avoid a recursive crawl on each URL.</p>
</li>
<li>
<p dir="auto"><code>--igon</code>: Print all URLs being ignored to the terminal and dashboard.  Can be
changed during the crawl by <code>touch</code>ing or <code>rm</code>ing the <code>DIR/igoff</code> file.
This is slower because it needs to find the specific regexp to blame.</p>
</li>
<li>
<p dir="auto"><code>--no-video</code>: Skip the download of videos by both mime type and file extension.
Skipped videos are logged to <code>DIR/skipped_videos</code>.  Can be
changed during the crawl by <code>touch</code>ing or <code>rm</code>ing the <code>DIR/video</code> file.</p>
</li>
<li>
<p dir="auto"><code>--no-sitemaps</code>: don't queue URLs from <code>sitemap.xml</code> at the root of the site.</p>
</li>
<li>
<p dir="auto"><code>--max-content-length=N</code>: Skip the download of any response that claims a
Content-Length larger than <code>N</code>.  (default: -1, don't skip anything).
Skipped URLs are logged to <code>DIR/skipped_max_content_length</code>.  Can be changed
during the crawl by editing the <code>DIR/max_content_length</code> file.</p>
</li>
<li>
<p dir="auto"><code>--no-dupespotter</code>: Disable dupespotter, a plugin that skips the extraction
of links from pages that look like duplicates of earlier pages.  Disable this
for sites that are directory listings, because they frequently trigger false
positives.</p>
</li>
<li>
<p dir="auto"><code>--concurrency=N</code>: Use <code>N</code> connections to fetch in parallel (default: 2).
Can be changed during the crawl by editing the <code>DIR/concurrency</code> file.</p>
</li>
<li>
<p dir="auto"><code>--delay=N</code>: Wait <code>N</code> milliseconds (default: 0) between requests on each concurrent fetcher.
Can be a range like X-Y to use a random delay between X and Y.  Can be changed during
the crawl by editing the <code>DIR/delay</code> file.</p>
</li>
<li>
<p dir="auto"><code>--import-ignores</code>: Copy this file to to <code>DIR/ignores</code> before the crawl begins.</p>
</li>
<li>
<p dir="auto"><code>--warc-max-size=BYTES</code>: Try to limit each WARC file to around <code>BYTES</code> bytes
before rolling over to a new WARC file (default: 5368709120, which is 5GiB).
Note that the resulting WARC files may be drastically larger if there are very
large responses.</p>
</li>
<li>
<p dir="auto"><code>--level=N</code>: recurse <code>N</code> levels instead of <code>inf</code> levels.</p>
</li>
<li>
<p dir="auto"><code>--page-requisites-level=N</code>: recurse page requisites <code>N</code> levels instead of <code>5</code> levels.</p>
</li>
<li>
<p dir="auto"><code>--ua=STRING</code>: Send User-Agent: <code>STRING</code> instead of pretending to be Firefox on Windows.</p>
</li>
<li>
<p dir="auto"><code>--id=ID</code>: Use id <code>ID</code> for the crawl instead of a random 128-bit id. This must be unique for every crawl.</p>
</li>
<li>
<p dir="auto"><code>--dir=DIR</code>: Put control files, temporary files, and unfinished WARCs in <code>DIR</code>
(default: a directory name based on the URL, date, and first 8 characters of the id).</p>
</li>
<li>
<p dir="auto"><code>--finished-warc-dir=FINISHED_WARC_DIR</code>: absolute path to a directory into
which finished <code>.warc.gz</code> and <code>.cdx</code> files will be moved.</p>
</li>
<li>
<p dir="auto"><code>--permanent-error-status-codes=STATUS_CODES</code>: A comma-separated list of
HTTP status codes to treat as a permanent error and therefore <strong>not</strong> retry
(default: <code>401,403,404,405,410</code>).  Other error responses tried another 2
times for a total of 3 tries (customizable with <code>--wpull-args=--tries=N</code>).
Note that, unlike wget, wpull puts retries at the end of the queue.</p>
</li>
<li>
<p dir="auto"><code>--wpull-args=ARGS</code>: String containing additional arguments to pass to wpull;
see <code>wpull --help</code>.  <code>ARGS</code> is split with <code>shlex.split</code> and individual
arguments can contain spaces if quoted, e.g.
<code>--wpull-args="--youtube-dl \"--youtube-dl-exe=/My Documents/youtube-dl\""</code></p>
<p dir="auto">Examples:</p>
<ul dir="auto">
<li><code>--wpull-args=--no-skip-getaddrinfo</code> to respect <code>/etc/hosts</code> entries.</li>
<li><code>--wpull-args=--no-warc-compression</code> to write uncompressed WARC files.</li>
</ul>
</li>
<li>
<p dir="auto"><code>--which-wpull-args-partial</code>: Print a partial list of wpull arguments that
would be used and exit.  Excludes grab-site-specific features, and removes
<code>DIR/</code> from paths.  Useful for reporting bugs on wpull without grab-site involvement.</p>
</li>
<li>
<p dir="auto"><code>--which-wpull-command</code>: Populate <code>DIR/</code> but don't start wpull; instead print
the command that would have been used to start wpull with all of the
grab-site functionality.</p>
</li>
<li>
<p dir="auto"><code>--debug</code>: print a lot of debug information.</p>
</li>
<li>
<p dir="auto"><code>--help</code>: print help text.</p>
</li>
</ul>
<h3 dir="auto">Warnings</h3>
<p dir="auto">If you pay no attention to your crawls, a crawl may head down some infinite bot
trap and stay there forever.  The site owner may eventually notice high CPU use
or log activity, then IP-ban you.</p>
<p dir="auto">grab-site does not respect <code>robots.txt</code> files, because they frequently
<a href="https://github.com/robots.txt">whitelist only approved robots</a>,
<a href="https://web.archive.org/web/20140401024610/http://www.thecrimson.com/robots.txt" rel="nofollow">hide pages embarrassing to the site owner</a>,
or block image or stylesheet resources needed for proper archival.
<a href="https://www.archiveteam.org/index.php?title=Robots.txt" rel="nofollow">See also</a>.
Because of this, very rarely you might run into a robot honeypot and receive
an abuse@ complaint.  Your host may require a prompt response to such a complaint
for your server to stay online.  Therefore, we recommend against crawling the
web from a server that hosts your critical infrastructure.</p>
<p dir="auto">Don't run grab-site on GCE (Google Compute Engine); as happened to me, your
entire API project may get nuked after a few days of crawling the web, with
no recourse.  Good alternatives include OVH (<a href="https://www.ovh.com/us/dedicated-servers/" rel="nofollow">OVH</a>,
<a href="https://www.soyoustart.com/us/essential-servers/" rel="nofollow">So You Start</a>,
<a href="https://www.kimsufi.com/us/en/index.xml" rel="nofollow">Kimsufi</a>), and online.net's
<a href="https://www.online.net/en/dedicated-server" rel="nofollow">dedicated</a> and
<a href="https://www.scaleway.com/" rel="nofollow">Scaleway</a> offerings.</p>
<h3 dir="auto">Tips for specific websites</h3>
<h4 dir="auto">Website requiring login / cookies</h4>
<p dir="auto">Log in to the website in Chrome or Firefox.  Use the cookies.txt extension
<a href="https://github.com/daftano/cookies.txt">for Chrome</a> or
<a href="https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/" rel="nofollow">for Firefox</a>
extension to copy Netscape-format cookies.  Paste the cookies data into a new
file.  Start grab-site with <code>--wpull-args=--load-cookies=ABSOLUTE_PATH_TO_COOKIES_FILE</code>.</p>
<h4 dir="auto">Static websites; WordPress blogs; Discourse forums</h4>
<p dir="auto">The defaults usually work fine.</p>
<h4 dir="auto">Blogger / blogspot.com blogs</h4>
<p dir="auto">The defaults work fine except for blogs with a JavaScript-only Dynamic Views theme.</p>
<p dir="auto">Some blogspot.com blogs use "<a href="https://support.google.com/blogger/answer/1229061?hl=en" rel="nofollow">Dynamic Views</a>"
themes that require JavaScript and serve absolutely no HTML content.  In rare
cases, you can get JavaScript-free pages by appending <code>?m=1</code>
(<a href="https://happinessbeyondthought.blogspot.com/?m=1" rel="nofollow">example</a>).  Otherwise, you
can archive parts of these blogs through Google Cache instead
(<a href="https://webcache.googleusercontent.com/search?q=cache:http://blog.datomic.com/" rel="nofollow">example</a>)
or by using <a href="https://archive.is/" rel="nofollow">https://archive.is/</a> instead of grab-site.</p>
<h4 dir="auto">Tumblr blogs</h4>
<p dir="auto">Either don't crawl from Europe (because tumblr redirects to a GDPR <code>/privacy/consent</code> page), or add <code>Googlebot</code> to the user agent:</p>
<div data-snippet-clipboard-copy-content="--ua &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/70.0 but not really nor Googlebot/2.1&quot;"><pre><code>--ua "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/70.0 but not really nor Googlebot/2.1"
</code></pre></div>
<p dir="auto">Use <a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/singletumblr"><code>--igsets=singletumblr</code></a>
to avoid crawling the homepages of other tumblr blogs.</p>
<p dir="auto">If you don't care about who liked or reblogged a post, add <code>\?from_c=</code> to the
crawl's <code>ignores</code>.</p>
<p dir="auto">Some tumblr blogs appear to require JavaScript, but they are actually just
hiding the page content with CSS.  You are still likely to get a complete crawl.
(See the links in the page source for <a href="https://x.tumblr.com/archive" rel="nofollow">https://X.tumblr.com/archive</a>).</p>
<h4 dir="auto">Subreddits</h4>
<p dir="auto">Use <a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/reddit"><code>--igsets=reddit</code></a>
and add a <code>/</code> at the end of the URL to avoid crawling all subreddits.</p>
<p dir="auto">When crawling a subreddit, you <strong>must</strong> get the casing of the subreddit right
for the recursive crawl to work.  For example,</p>
<div data-snippet-clipboard-copy-content="grab-site https://www.reddit.com/r/Oculus/ --igsets=reddit"><pre><code>grab-site https://www.reddit.com/r/Oculus/ --igsets=reddit
</code></pre></div>
<p dir="auto">will crawl only a few pages instead of the entire subreddit.  The correct casing is:</p>
<div data-snippet-clipboard-copy-content="grab-site https://www.reddit.com/r/oculus/ --igsets=reddit"><pre><code>grab-site https://www.reddit.com/r/oculus/ --igsets=reddit
</code></pre></div>
<p dir="auto">You can hover over the "Hot"/"New"/... links at the top of the page to see the correct casing.</p>
<h4 dir="auto">Directory listings ("Index of ...")</h4>
<p dir="auto">Use <code>--no-dupespotter</code> to avoid triggering false positives on the duplicate
page detector.  Without it, the crawl may miss large parts of the directory tree.</p>
<h4 dir="auto">Very large websites</h4>
<p dir="auto">Use <code>--no-offsite-links</code> to stay on the main website and avoid crawling linked pages on other domains.</p>
<h4 dir="auto">Websites that are likely to ban you for crawling fast</h4>
<p dir="auto">Use <code>--concurrency=1 --delay=500-1500</code>.</p>
<h4 dir="auto">MediaWiki sites with English language</h4>
<p dir="auto">Use <a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/mediawiki"><code>--igsets=mediawiki</code></a>.
Note that this ignore set ignores old page revisions.</p>
<h4 dir="auto">MediaWiki sites with non-English language</h4>
<p dir="auto">You will probably have to add ignores with translated <code>Special:*</code> URLs based on
<a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/mediawiki">ignore_sets/mediawiki</a>.</p>
<h4 dir="auto">Forums that aren't Discourse</h4>
<p dir="auto">Forums require more manual intervention with ignore patterns.
<a href="https://github.com/ArchiveTeam/grab-site/blob/master/libgrabsite/ignore_sets/forums"><code>--igsets=forums</code></a>
is often useful for non-SMF forums, but you will have to add other ignore
patterns, including one to ignore individual-forum-post pages if there are
too many posts to crawl.  (Generally, crawling the thread pages is enough.)</p>
<h4 dir="auto">GitHub issues / pull requests</h4>
<p dir="auto">Find the highest issue number from an issues page (<a href="https://github.com/rust-lang/rust/issues">example</a>) and use:</p>
<div data-snippet-clipboard-copy-content="grab-site --1 https://github.com/rust-lang/rust/issues/{1..30000}"><pre><code>grab-site --1 https://github.com/rust-lang/rust/issues/{1..30000}
</code></pre></div>
<p dir="auto">This relies on your shell to expand the argument to thousands of arguments.
If there are too many arguments, you may have to write the URLs to a file
and use <code>grab-site -i</code> instead:</p>
<div data-snippet-clipboard-copy-content="for i in {1..30000}; do echo https://github.com/rust-lang/rust/issues/$i >> .urls; done
grab-site --1 -i .urls"><pre><code>for i in {1..30000}; do echo https://github.com/rust-lang/rust/issues/$i &gt;&gt; .urls; done
grab-site --1 -i .urls
</code></pre></div>
<h4 dir="auto">Websites whose domains have just expired but are still up at the webhost</h4>
<p dir="auto">Use a <a href="https://www.google.com/search?q=historical+OR+history+dns" rel="nofollow">DNS history</a>
service to find the old IP address (the DNS "A" record) for the domain.  Add a
line to your <code>/etc/hosts</code> to point the domain to the old IP.  Start a crawl
with <code>--wpull-args=--no-skip-getaddrinfo</code> to make wpull use <code>/etc/hosts</code>.</p>
<h4 dir="auto">twitter.com/user</h4>
<p dir="auto">Use <a href="https://github.com/JustAnotherArchivist/snscrape">snscrape</a> to get a list
of tweets for a user.  Redirect <code>snscrape</code>'s output to a list of URLs with
<code>&gt; urls</code> and pass this file to <code>grab-site --1 -i urls</code>.</p>
<p dir="auto">Alternatively, use <a href="https://webrecorder.io/" rel="nofollow">webrecorder.io</a> instead of
grab-site.  It has an autoscroll feature and you can download the WARCs.</p>
<p dir="auto">Keep in mind that scrolling <code>twitter.com/user</code> returns a maximum of 3200 tweets,
while a <a href="https://twitter.com/search?q=from%3Ainternetarchive&amp;src=typd&amp;f=realtime&amp;qf=off&amp;lang=en" rel="nofollow">from:user</a>
query can return more.</p>
<h2 dir="auto">Changing ignores during the crawl</h2>
<p dir="auto">While the crawl is running, you can edit <code>DIR/ignores</code> and <code>DIR/igsets</code>; the
changes will be applied within a few seconds.</p>
<p dir="auto"><code>DIR/igsets</code> is a comma-separated list of ignore sets to use.</p>
<p dir="auto"><code>DIR/ignores</code> is a newline-separated list of <a href="https://pythex.org/" rel="nofollow">Python 3 regular expressions</a>
to use in addition to the ignore sets.</p>
<p dir="auto">You can <code>rm DIR/igoff</code> to display all URLs that are being filtered out
by the ignores, and <code>touch DIR/igoff</code> to turn it back off.</p>
<p dir="auto">Note that ignores will not apply to any of the crawl's start URLs.</p>
<h2 dir="auto">Inspecting the URL queue</h2>
<p dir="auto">Inspecting the URL queue is usually not necessary, but may be helpful
for adding ignores before grab-site crawls a large number of junk URLs.</p>
<p dir="auto">To dump the queue, run:</p>
<div data-snippet-clipboard-copy-content="gs-dump-urls DIR/wpull.db todo"><pre><code>gs-dump-urls DIR/wpull.db todo
</code></pre></div>
<p dir="auto">Four other statuses can be used besides <code>todo</code>:
<code>done</code>, <code>error</code>, <code>in_progress</code>, and <code>skipped</code>.</p>
<p dir="auto">You may want to pipe the output to <code>sort</code> and <code>less</code>:</p>
<div data-snippet-clipboard-copy-content="gs-dump-urls DIR/wpull.db todo | sort | less -S"><pre><code>gs-dump-urls DIR/wpull.db todo | sort | less -S
</code></pre></div>
<h2 dir="auto">Preventing a crawl from queuing any more URLs</h2>
<p dir="auto"><code>rm DIR/scrape</code>.  Responses will no longer be scraped for URLs.  Scraping cannot
be re-enabled for a crawl.</p>
<h2 dir="auto">Stopping a crawl</h2>
<p dir="auto">You can <code>touch DIR/stop</code> or press ctrl-c, which will do the same.  You will
have to wait for the current downloads to finish.</p>
<h2 dir="auto">Advanced <code>gs-server</code> options</h2>
<p dir="auto">These environmental variables control what <code>gs-server</code> listens on:</p>
<ul dir="auto">
<li><code>GRAB_SITE_INTERFACE</code> (default <code>0.0.0.0</code>)</li>
<li><code>GRAB_SITE_PORT</code> (default <code>29000</code>)</li>
</ul>
<p dir="auto">These environmental variables control which server each <code>grab-site</code> process connects to:</p>
<ul dir="auto">
<li><code>GRAB_SITE_HOST</code> (default <code>127.0.0.1</code>)</li>
<li><code>GRAB_SITE_PORT</code> (default <code>29000</code>)</li>
</ul>
<h2 dir="auto">Viewing the content in your WARC archives</h2>
<p dir="auto">Try <a href="https://replayweb.page/" rel="nofollow">ReplayWeb.page</a> or <a href="https://github.com/webrecorder/webrecorder-player">webrecorder-player</a>.</p>
<h2 dir="auto">Inspecting WARC files in the terminal</h2>
<p dir="auto"><code>zless</code> is a wrapper over <code>less</code> that can be used to view raw WARC content:</p>

<p dir="auto"><code>zless -S</code> will turn off line wrapping.</p>
<p dir="auto">Note that grab-site requests uncompressed HTTP responses to avoid
double-compression in .warc.gz files and to make zless output more useful.
However, some servers will send compressed responses anyway.</p>
<h2 dir="auto">Automatically pausing grab-site processes when free disk is low</h2>
<p dir="auto">If you automatically upload and remove finished .warc.gz files, you can still
run into a situation where grab-site processes fill up your disk faster than
your uploader process can handle.  To prevent this situation, you can customize
and run <a href="https://github.com/ArchiveTeam/grab-site/blob/master/extra_docs/pause_resume_grab_sites.sh">this script</a>,
which will pause and resume grab-site processes as your free disk space
crosses a threshold value.</p>
<h2 dir="auto">Thanks</h2>
<p dir="auto">grab-site is made possible only because of <a href="https://github.com/chfoo/wpull">wpull</a>,
written by <a href="https://github.com/chfoo">Christopher Foo</a> who spent a year
making something much better than wget.  ArchiveTeam's most pressing
issue with wget at the time was that it kept the entire URL queue in memory
instead of on disk.  wpull has many other advantages over wget, including
better link extraction and Python hooks.</p>
<p dir="auto">Thanks to <a href="https://github.com/yipdw">David Yip</a>, who created
<a href="https://github.com/ArchiveTeam/ArchiveBot">ArchiveBot</a>.  The wpull
hooks in ArchiveBot served as the basis for grab-site.  The original ArchiveBot
dashboard inspired the newer dashboard now used in both projects.</p>
<p dir="auto">Thanks to <a href="https://github.com/falconkirtaran">Falcon Darkstar Momot</a> for
the many wpull 2.x fixes that were rolled into
<a href="https://github.com/ArchiveTeam/wpull">ArchiveTeam/wpull</a>.</p>
<p dir="auto">Thanks to <a href="https://github.com/JustAnotherArchivist">JustAnotherArchivist</a>
for investigating my wpull issues.</p>
<h2 dir="auto">Help</h2>
<p dir="auto">grab-site bugs and questions are welcome in
<a href="https://github.com/ArchiveTeam/grab-site/issues">grab-site/issues</a>.</p>
<p dir="auto">Terminal output in your bug report should be surrounded by triple backquotes, like this:</p>
<pre>```
very
long
output
```
</pre>
<p dir="auto">Please report security bugs as regular bugs.</p>
</article>
        </div></div>
                
                </main>
                <footer>

                </footer>
            </div>
            <!--[if !IE]><script>fixScale(document);</script><![endif]-->
        </body>
    </html>
